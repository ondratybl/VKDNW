{
 "cells": [
  {
   "cell_type": "code",
   "id": "c4cc63dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T10:29:13.155595Z",
     "start_time": "2024-10-17T10:29:10.154541Z"
    }
   },
   "source": [
    "import os, sys, time, glob, random, argparse\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import tqdm\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# XAutoDL \n",
    "from xautodl.config_utils import load_config, dict2config, configure2str\n",
    "from xautodl.datasets import get_datasets, get_nas_search_loaders\n",
    "from xautodl.procedures import (\n",
    "    prepare_seed,\n",
    "    prepare_logger,\n",
    "    save_checkpoint,\n",
    "    copy_checkpoint,\n",
    "    get_optim_scheduler,\n",
    ")\n",
    "from xautodl.utils import get_model_infos, obtain_accuracy\n",
    "from xautodl.log_utils import AverageMeter, time_string, convert_secs2time\n",
    "from xautodl.models import get_search_spaces\n",
    "\n",
    "# API\n",
    "from nats_bench import create\n",
    "\n",
    "# custom modules\n",
    "from custom.tss_model import TinyNetwork\n",
    "from xautodl.models.cell_searchs.genotypes import Structure\n",
    "from ZeroShotProxy import *\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "6018e93f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T10:29:13.168089Z",
     "start_time": "2024-10-17T10:29:13.157569Z"
    }
   },
   "source": [
    "parser = argparse.ArgumentParser(\"Training-free NAS on NAS-Bench-201 (NATS-Bench-TSS)\")\n",
    "parser.add_argument(\"--data_path\", type=str, default='./cifar.python', help=\"The path to dataset\")\n",
    "parser.add_argument(\"--dataset\", type=str, default='cifar10',choices=[\"cifar10\", \"cifar100\", \"ImageNet16-120\"], help=\"Choose between Cifar10/100 and ImageNet-16.\")\n",
    "\n",
    "# channels and number-of-cells\n",
    "parser.add_argument(\"--search_space\", type=str, default='tss', help=\"The search space name.\")\n",
    "parser.add_argument(\"--config_path\", type=str, default='./configs/nas-benchmark/algos/weight-sharing.config', help=\"The path to the configuration.\")\n",
    "parser.add_argument(\"--max_nodes\", type=int, default=4, help=\"The maximum number of nodes.\")\n",
    "parser.add_argument(\"--channel\", type=int, default=16, help=\"The number of channels.\")\n",
    "parser.add_argument(\"--num_cells\", type=int, default=5, help=\"The number of cells in one stage.\")\n",
    "parser.add_argument(\"--affine\", type=int, default=1, choices=[0, 1], help=\"Whether use affine=True or False in the BN layer.\")\n",
    "parser.add_argument(\"--track_running_stats\", type=int, default=0, choices=[0, 1], help=\"Whether use track_running_stats or not in the BN layer.\")\n",
    "\n",
    "# log\n",
    "parser.add_argument(\"--print_freq\", type=int, default=200, help=\"print frequency (default: 200)\")\n",
    "\n",
    "# custom\n",
    "parser.add_argument(\"--gpu\", type=int, default=0, help=\"\")\n",
    "parser.add_argument(\"--workers\", type=int, default=4, help=\"number of data loading workers\")\n",
    "#parser.add_argument(\"--api_data_path\", type=str, default=\"/mnt/personal/tyblondr/NATS-tss-v1_0-3ffb9-full/data/NATS-tss-v1_0-3ffb9-full\", help=\"\")\n",
    "parser.add_argument(\"--api_data_path\", type=str, default=\"/mnt/personal/tyblondr/NATS-tss-v1_0-3ffb9-simple/\", help=\"\")\n",
    "parser.add_argument(\"--save_dir\", type=str, default='./results/tmp', help=\"Folder to save checkpoints and log.\")\n",
    "parser.add_argument('--zero_shot_score', type=str, default='vkdnw', choices=['az_nas','zico','zen','gradnorm','naswot','synflow','snip','grasp','te_nas','gradsign'])\n",
    "parser.add_argument(\"--rand_seed\", type=int, default=1, help=\"manual seed (we use 1-to-5)\")\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "if args.rand_seed is None or args.rand_seed < 0:\n",
    "    args.rand_seed = random.randint(1, 100000)\n",
    "\n",
    "print(args.rand_seed)\n",
    "print(args)\n",
    "xargs=args"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Namespace(affine=1, api_data_path='/mnt/personal/tyblondr/NATS-tss-v1_0-3ffb9-simple/', channel=16, config_path='./configs/nas-benchmark/algos/weight-sharing.config', data_path='./cifar.python', dataset='cifar10', gpu=0, max_nodes=4, num_cells=5, print_freq=200, rand_seed=1, save_dir='./results/tmp', search_space='tss', track_running_stats=0, workers=4, zero_shot_score='vkdnw')\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "f8a05319",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T10:29:14.604713Z",
     "start_time": "2024-10-17T10:29:13.169731Z"
    }
   },
   "source": [
    "assert torch.cuda.is_available(), \"CUDA is not available.\"\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.set_num_threads(xargs.workers)\n",
    "prepare_seed(xargs.rand_seed)\n",
    "logger = prepare_logger(args)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Function with logger : Logger(dir=results/tmp, use-tf=False, writer=None)\n",
      "Arguments : -------------------------------\n",
      "affine           : 1\n",
      "api_data_path    : /mnt/personal/tyblondr/NATS-tss-v1_0-3ffb9-simple/\n",
      "channel          : 16\n",
      "config_path      : ./configs/nas-benchmark/algos/weight-sharing.config\n",
      "data_path        : ./cifar.python\n",
      "dataset          : cifar10\n",
      "gpu              : 0\n",
      "max_nodes        : 4\n",
      "num_cells        : 5\n",
      "print_freq       : 200\n",
      "rand_seed        : 1\n",
      "save_dir         : ./results/tmp\n",
      "search_space     : tss\n",
      "track_running_stats : 0\n",
      "workers          : 4\n",
      "zero_shot_score  : vkdnw\n",
      "Python  Version  : 3.8.20 (default, Oct  3 2024, 15:24:27)  [GCC 11.2.0]\n",
      "Pillow  Version  : 10.4.0\n",
      "PyTorch Version  : 2.0.1\n",
      "cuDNN   Version  : 8500\n",
      "CUDA available   : True\n",
      "CUDA GPU numbers : 1\n",
      "CUDA_VISIBLE_DEVICES : 0\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "78557cdb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T10:29:16.172593Z",
     "start_time": "2024-10-17T10:29:14.607484Z"
    }
   },
   "source": [
    "## API\n",
    "api = create(xargs.api_data_path, xargs.search_space, fast_mode=True, verbose=False)\n",
    "logger.log(\"Create API = {:} done\".format(api))\n",
    "\n",
    "## data\n",
    "train_data, valid_data, xshape, class_num = get_datasets(xargs.dataset, xargs.data_path, -1)\n",
    "config = load_config(xargs.config_path, {\"class_num\": class_num, \"xshape\": xshape}, logger)\n",
    "search_loader, train_loader, valid_loader = get_nas_search_loaders(train_data,\n",
    "                                                                   valid_data,\n",
    "                                                                   xargs.dataset,\n",
    "                                                                   \"./configs/nas-benchmark/\",\n",
    "                                                                   (config.batch_size, config.test_batch_size),\n",
    "                                                                   xargs.workers,)\n",
    "logger.log(\"||||||| {:10s} ||||||| Search-Loader-Num={:}, Valid-Loader-Num={:}, batch size={:}\".format(xargs.dataset, len(search_loader), len(valid_loader), config.batch_size))\n",
    "logger.log(\"||||||| {:10s} ||||||| Config={:}\".format(xargs.dataset, config))\n",
    "\n",
    "## model\n",
    "search_space = get_search_spaces(xargs.search_space, \"nats-bench\")\n",
    "logger.log(\"search space : {:}\".format(search_space))\n",
    "\n",
    "device = torch.device('cuda:{}'.format(xargs.gpu))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create API = NATStopology(0/15625 architectures, fast_mode=True, file=) done\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "./configs/nas-benchmark/algos/weight-sharing.config\n",
      "Configure(scheduler='cos', LR=0.025, eta_min=0.001, epochs=100, warmup=0, optim='SGD', decay=0.0005, momentum=0.9, nesterov=True, criterion='Softmax', batch_size=64, test_batch_size=512, class_num=10, xshape=(1, 3, 32, 32))\n",
      "||||||| cifar10    ||||||| Search-Loader-Num=391, Valid-Loader-Num=49, batch size=64\n",
      "||||||| cifar10    ||||||| Config=Configure(scheduler='cos', LR=0.025, eta_min=0.001, epochs=100, warmup=0, optim='SGD', decay=0.0005, momentum=0.9, nesterov=True, criterion='Softmax', batch_size=64, test_batch_size=512, class_num=10, xshape=(1, 3, 32, 32))\n",
      "search space : ['none', 'skip_connect', 'nor_conv_1x1', 'nor_conv_3x3', 'avg_pool_3x3']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyblondr/.conda/envs/VKDNW/lib/python3.8/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "c557c0f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T10:29:16.191954Z",
     "start_time": "2024-10-17T10:29:16.174298Z"
    }
   },
   "source": [
    "def random_genotype(max_nodes, op_names):\n",
    "    genotypes = []\n",
    "    for i in range(1, max_nodes):\n",
    "        xlist = []\n",
    "        for j in range(i):\n",
    "            node_str = \"{:}<-{:}\".format(i, j)\n",
    "            op_name = random.choice(op_names)\n",
    "            xlist.append((op_name, j))\n",
    "        genotypes.append(tuple(xlist))\n",
    "    arch = Structure(genotypes)\n",
    "    return arch\n",
    "\n",
    "real_input_metrics = ['zico', 'snip', 'grasp', 'te_nas', 'gradsign', 'jacov']\n",
    "    \n",
    "def search_find_best(xargs, xloader, n_samples = None, archs = None):\n",
    "    logger.log(\"Searching with {}\".format(xargs.zero_shot_score.lower()))\n",
    "    score_fn_name = \"compute_{}_score\".format(xargs.zero_shot_score.lower())\n",
    "    score_fn = globals().get(score_fn_name)\n",
    "    input_, target_ = next(iter(xloader))\n",
    "    resolution = input_.size(2)\n",
    "    batch_size = input_.size(0)\n",
    "    zero_shot_score_dict = None\n",
    "    arch_list = []\n",
    "    if xargs.zero_shot_score.lower() in real_input_metrics:\n",
    "        print('Use real images as inputs')\n",
    "        trainloader = train_loader\n",
    "    else:\n",
    "        print('Use random inputs')\n",
    "        trainloader = None\n",
    "        \n",
    "    if archs is None and n_samples is not None:\n",
    "        all_time = []\n",
    "        all_mem = []\n",
    "        start = torch.cuda.Event(enable_timing=True)\n",
    "        end = torch.cuda.Event(enable_timing=True)\n",
    "        for i in tqdm.tqdm(range(n_samples)):\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            # random sampling\n",
    "            arch = random_genotype(xargs.max_nodes, search_space)\n",
    "            network = TinyNetwork(xargs.channel, xargs.num_cells, arch, class_num)\n",
    "            network = network.to(device)\n",
    "            network.train()\n",
    "\n",
    "            start.record()\n",
    "            \n",
    "\n",
    "            info_dict = score_fn.compute_nas_score(network, gpu=xargs.gpu, trainloader=trainloader, resolution=resolution, batch_size=batch_size)\n",
    "\n",
    "            end.record()\n",
    "            torch.cuda.synchronize()\n",
    "            all_time.append(start.elapsed_time(end))\n",
    "#             all_mem.append(torch.cuda.max_memory_reserved())\n",
    "            all_mem.append(torch.cuda.max_memory_allocated())\n",
    "\n",
    "            arch_list.append(arch)\n",
    "            if zero_shot_score_dict is None: # initialize dict\n",
    "                zero_shot_score_dict = dict()\n",
    "                for k in info_dict.keys():\n",
    "                    zero_shot_score_dict[k] = []\n",
    "            for k, v in info_dict.items():\n",
    "                zero_shot_score_dict[k].append(v)\n",
    "\n",
    "        logger.log(\"------Runtime------\")\n",
    "        logger.log(\"All: {:.5f} ms\".format(np.mean(all_time)))\n",
    "        logger.log(\"------Avg Mem------\")\n",
    "        logger.log(\"All: {:.5f} GB\".format(np.mean(all_mem)/1e9))\n",
    "        logger.log(\"------Max Mem------\")\n",
    "        logger.log(\"All: {:.5f} GB\".format(np.max(all_mem)/1e9))\n",
    "        \n",
    "    elif archs is not None and n_samples is None:\n",
    "        all_time = []\n",
    "        all_mem = []\n",
    "        start = torch.cuda.Event(enable_timing=True)\n",
    "        end = torch.cuda.Event(enable_timing=True)\n",
    "        for arch in tqdm.tqdm(archs):\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            network = TinyNetwork(xargs.channel, xargs.num_cells, arch, class_num)\n",
    "            network = network.to(device)\n",
    "            network.train()\n",
    "\n",
    "            start.record()\n",
    "            \n",
    "\n",
    "            info_dict = score_fn.compute_nas_score(network, gpu=xargs.gpu, trainloader=trainloader, resolution=resolution, batch_size=batch_size)\n",
    "\n",
    "            end.record()\n",
    "            torch.cuda.synchronize()\n",
    "            all_time.append(start.elapsed_time(end))\n",
    "#             all_mem.append(torch.cuda.max_memory_reserved())\n",
    "            all_mem.append(torch.cuda.max_memory_allocated())\n",
    "\n",
    "            arch_list.append(arch)\n",
    "            if zero_shot_score_dict is None: # initialize dict\n",
    "                zero_shot_score_dict = dict()\n",
    "                for k in info_dict.keys():\n",
    "                    zero_shot_score_dict[k] = []\n",
    "            for k, v in info_dict.items():\n",
    "                zero_shot_score_dict[k].append(v)\n",
    "\n",
    "        logger.log(\"------Runtime------\")\n",
    "        logger.log(\"All: {:.5f} ms\".format(np.mean(all_time)))\n",
    "        logger.log(\"------Avg Mem------\")\n",
    "        logger.log(\"All: {:.5f} GB\".format(np.mean(all_mem)/1e9))\n",
    "        logger.log(\"------Max Mem------\")\n",
    "        logger.log(\"All: {:.5f} GB\".format(np.max(all_mem)/1e9))\n",
    "        \n",
    "    return arch_list, zero_shot_score_dict"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "2967c488",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T10:29:16.196686Z",
     "start_time": "2024-10-17T10:29:16.193478Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "562c71e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T10:29:20.046615Z",
     "start_time": "2024-10-17T10:29:16.198317Z"
    }
   },
   "source": [
    "# ######### search across random N archs #########\n",
    "# archs, results = search_find_best(xargs, train_loader, n_samples=3000)\n",
    "\n",
    "# ######### search across N archs uniformly sampled according to test acc. #########\n",
    "# def uniform_sample_archs(search_space, xargs, api, n_samples=1000, dataset='ImageNet16-120'):\n",
    "#     arch = random_genotype(xargs.max_nodes, search_space)\n",
    "#     search_space = get_search_spaces(xargs.search_space, \"nats-bench\")\n",
    "#     archs = arch.gen_all(search_space, xargs.max_nodes, False)\n",
    "    \n",
    "#     def get_results_from_api(api, arch, dataset='cifar10'):\n",
    "#         dataset_candidates = ['cifar10-valid', 'cifar10', 'cifar100', 'ImageNet16-120']\n",
    "#         assert dataset in dataset_candidates\n",
    "#         index = api.query_index_by_arch(arch)\n",
    "#         api._prepare_info(index)\n",
    "#         archresult = api.arch2infos_dict[index]['200']\n",
    "#         if dataset == 'cifar10-valid':\n",
    "#             acc = archresult.get_metrics(dataset, 'x-valid', iepoch=None, is_random=False)['accuracy']\n",
    "#         elif dataset == 'cifar10':\n",
    "#             acc = archresult.get_metrics(dataset, 'ori-test', iepoch=None, is_random=False)['accuracy']\n",
    "#         else:\n",
    "#             acc = archresult.get_metrics(dataset, 'x-test', iepoch=None, is_random=False)['accuracy']\n",
    "#         return acc\n",
    "\n",
    "#     accs = []\n",
    "#     for a in archs:\n",
    "#         accs.append(get_results_from_api(api, a, dataset))\n",
    "#     interval = len(archs) // n_samples\n",
    "#     sorted_indices = np.argsort(accs)\n",
    "#     new_archs = []\n",
    "#     for i, idx in enumerate(sorted_indices):\n",
    "#         if i % interval == 0:\n",
    "#             new_archs.append(archs[idx])\n",
    "#     archs = new_archs\n",
    "    \n",
    "#     return archs\n",
    "\n",
    "# if os.path.exists(\"./tss_uniform_arch.pickle\"):\n",
    "#     with open(\"./tss_uniform_arch.pickle\", \"rb\") as fp:\n",
    "#         uniform_archs = pickle.load(fp)\n",
    "# else:\n",
    "#     uniform_archs = uniform_sample_archs(search_space, xargs, api, 1000, 'ImageNet16-120')\n",
    "#     with open(\"./tss_uniform_arch.pickle\", \"wb\") as fp:\n",
    "#         pickle.dump(uniform_archs, fp)\n",
    "        \n",
    "# result_path = \"./{}_uniform_arch.pickle\".format(args.zero_shot_score)\n",
    "# if os.path.exists(result_path):\n",
    "#     print(\"results already exists\")\n",
    "#     with open(result_path, \"rb\") as fp:\n",
    "#         results = pickle.load(fp)\n",
    "#     archs = uniform_archs\n",
    "# else:\n",
    "#     archs, results = search_find_best(xargs, train_loader, archs=uniform_archs)\n",
    "#     with open(result_path, \"wb\") as fp:\n",
    "#         pickle.dump(results, fp)\n",
    "\n",
    "\n",
    "######### search across all archs #########\n",
    "def generate_all_archs(search_space, xargs):\n",
    "    arch = random_genotype(xargs.max_nodes, search_space)\n",
    "    archs = arch.gen_all(search_space, xargs.max_nodes, False)\n",
    "    return archs\n",
    "\n",
    "if os.path.exists(\"./tss_all_arch.pickle\"):\n",
    "    with open(\"./tss_all_arch.pickle\", \"rb\") as fp:\n",
    "        all_archs = pickle.load(fp)\n",
    "else:\n",
    "    all_archs = generate_all_archs(search_space, xargs)\n",
    "    with open(\"./tss_all_arch.pickle\", \"wb\") as fp:\n",
    "        pickle.dump(all_archs, fp)\n",
    "\n",
    "# archs, results = search_find_best(xargs, train_loader, archs=all_archs)\n",
    "\n",
    "####\n",
    "for zero_shot_score in ['jacov', 'vkdnw', 'az_nas', 'gradsign', 'zico', 'zen','gradnorm','naswot','synflow','snip','grasp','te_nas']:\n",
    "    xargs.zero_shot_score = zero_shot_score\n",
    "    result_path = \"./{}_all_arch.pickle\".format(xargs.zero_shot_score)\n",
    "    if os.path.exists(result_path):\n",
    "        print(\"results already exists\")\n",
    "        with open(result_path, \"rb\") as fp:\n",
    "            results = pickle.load(fp)\n",
    "        archs = all_archs\n",
    "    else:\n",
    "        archs, results = search_find_best(xargs, train_loader, archs=all_archs)\n",
    "        with open(result_path, \"wb\") as fp:\n",
    "            pickle.dump(results, fp)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching with jacov\n",
      "Use real images as inputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15625 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'ZeroShotProxy.compute_jacov_score' has no attribute 'compute_nas_score'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 83\u001B[0m\n\u001B[1;32m     81\u001B[0m     archs \u001B[38;5;241m=\u001B[39m all_archs\n\u001B[1;32m     82\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 83\u001B[0m     archs, results \u001B[38;5;241m=\u001B[39m \u001B[43msearch_find_best\u001B[49m\u001B[43m(\u001B[49m\u001B[43mxargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marchs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mall_archs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     84\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(result_path, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwb\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m fp:\n\u001B[1;32m     85\u001B[0m         pickle\u001B[38;5;241m.\u001B[39mdump(results, fp)\n",
      "Cell \u001B[0;32mIn[5], line 86\u001B[0m, in \u001B[0;36msearch_find_best\u001B[0;34m(xargs, xloader, n_samples, archs)\u001B[0m\n\u001B[1;32m     81\u001B[0m network\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[1;32m     83\u001B[0m start\u001B[38;5;241m.\u001B[39mrecord()\n\u001B[0;32m---> 86\u001B[0m info_dict \u001B[38;5;241m=\u001B[39m \u001B[43mscore_fn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_nas_score\u001B[49m(network, gpu\u001B[38;5;241m=\u001B[39mxargs\u001B[38;5;241m.\u001B[39mgpu, trainloader\u001B[38;5;241m=\u001B[39mtrainloader, resolution\u001B[38;5;241m=\u001B[39mresolution, batch_size\u001B[38;5;241m=\u001B[39mbatch_size)\n\u001B[1;32m     88\u001B[0m end\u001B[38;5;241m.\u001B[39mrecord()\n\u001B[1;32m     89\u001B[0m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39msynchronize()\n",
      "\u001B[0;31mAttributeError\u001B[0m: module 'ZeroShotProxy.compute_jacov_score' has no attribute 'compute_nas_score'"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "dd1762a07e139cdd"
  },
  {
   "cell_type": "code",
   "id": "2d4147a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T10:24:04.252235Z",
     "start_time": "2024-10-17T10:24:04.252106Z"
    }
   },
   "source": [
    "def get_results_from_api(api, arch, dataset='cifar10'):\n",
    "    dataset_candidates = ['cifar10-valid', 'cifar10', 'cifar100', 'ImageNet16-120']\n",
    "    assert dataset in dataset_candidates\n",
    "    index = api.query_index_by_arch(arch)\n",
    "    api._prepare_info(index)\n",
    "    archresult = api.arch2infos_dict[index]['200']\n",
    "    \n",
    "    if dataset == 'cifar10-valid':\n",
    "        acc = archresult.get_metrics(dataset, 'x-valid', iepoch=None, is_random=False)['accuracy']\n",
    "    elif dataset == 'cifar10':\n",
    "        acc = archresult.get_metrics(dataset, 'ori-test', iepoch=None, is_random=False)['accuracy']\n",
    "    else:\n",
    "        acc = archresult.get_metrics(dataset, 'x-test', iepoch=None, is_random=False)['accuracy']\n",
    "    flops = archresult.get_compute_costs(dataset)['flops']\n",
    "    params = archresult.get_compute_costs(dataset)['params']\n",
    "    \n",
    "    return acc, flops, params\n",
    "\n",
    "api_valid_accs, api_flops, api_params = [], [], []\n",
    "for a in archs:\n",
    "    try:\n",
    "        valid_acc, flops, params = get_results_from_api(api, a, 'cifar10')\n",
    "    #     valid_acc, flops, params = get_results_from_api(api, a, 'cifar100')\n",
    "    #     valid_acc, flops, params = get_results_from_api(api, a, 'ImageNet16-120')\n",
    "        api_valid_accs.append(valid_acc)\n",
    "        api_flops.append(flops)\n",
    "        api_params.append(params)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "print(\"Maximum acc: {}% \\n Info\".format(np.max(api_valid_accs)))\n",
    "best_idx = np.argmax(api_valid_accs)\n",
    "best_arch = archs[best_idx]\n",
    "if api is not None:\n",
    "    print(\"{:}\".format(api.query_by_arch(best_arch, \"200\")))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "32bd185d",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2024-10-17T10:24:04.253977Z",
     "start_time": "2024-10-17T10:24:04.253646Z"
    }
   },
   "source": [
    "fig_scale = 1.1\n",
    "\n",
    "if xargs.zero_shot_score.lower() == 'az_nas':\n",
    "    rank_agg = None\n",
    "    l = len(api_flops)\n",
    "    rank_agg = np.log(stats.rankdata(api_flops) / l)\n",
    "    for k in results.keys():\n",
    "        print(k)\n",
    "        if rank_agg is None:\n",
    "            rank_agg = np.log( stats.rankdata(results[k]) / l)\n",
    "        else:\n",
    "            rank_agg = rank_agg + np.log( stats.rankdata(results[k]) / l)\n",
    "\n",
    "\n",
    "    best_idx = np.argmax(rank_agg)\n",
    "\n",
    "    best_arch, acc = archs[best_idx], api_valid_accs[best_idx]\n",
    "    if api is not None:\n",
    "        print(\"{:}\".format(api.query_by_arch(best_arch, \"200\")))\n",
    "\n",
    "\n",
    "    x = stats.rankdata(rank_agg)\n",
    "    y = stats.rankdata(api_valid_accs)\n",
    "    kendalltau = stats.kendalltau(x, y)\n",
    "    spearmanr = stats.spearmanr(x, y)\n",
    "    pearsonr = stats.pearsonr(x, y)\n",
    "    print(\"AZ-NAS: {}\\t{}\\t{}\\t\".format(kendalltau[0], pearsonr[0], spearmanr[0]))\n",
    "    plt.figure(figsize=(4*fig_scale,3*fig_scale))\n",
    "    plt.scatter(x, y, linewidths=0.1)\n",
    "    best_idx = np.argmax(rank_agg)\n",
    "    plt.scatter(x[best_idx], y[best_idx], c=\"r\", linewidths=0.1)\n",
    "    plt.title(\"AZ-NAS\")\n",
    "    plt.show()\n",
    "    \n",
    "elif xargs.zero_shot_score.lower() == 'te_nas':\n",
    "    rank_agg = None\n",
    "    for k in results.keys():\n",
    "        print(k)\n",
    "        if rank_agg is None:\n",
    "            rank_agg = stats.rankdata(results[k])\n",
    "        else:\n",
    "            rank_agg = rank_agg + stats.rankdata(results[k])\n",
    "\n",
    "\n",
    "    best_idx = np.argmax(rank_agg)\n",
    "\n",
    "    best_arch, acc = archs[best_idx], api_valid_accs[best_idx]\n",
    "    if api is not None:\n",
    "        print(\"{:}\".format(api.query_by_arch(best_arch, \"200\")))\n",
    "\n",
    "\n",
    "    x = stats.rankdata(rank_agg)\n",
    "    y = stats.rankdata(api_valid_accs)\n",
    "    kendalltau = stats.kendalltau(x, y)\n",
    "    spearmanr = stats.spearmanr(x, y)\n",
    "    pearsonr = stats.pearsonr(x, y)\n",
    "    print(\"TE-NAS: {}\\t{}\\t{}\\t\".format(kendalltau[0], pearsonr[0], spearmanr[0]))\n",
    "    plt.figure(figsize=(4*fig_scale,3*fig_scale))\n",
    "    plt.scatter(x, y, linewidths=0.1)\n",
    "    best_idx = np.argmax(rank_agg)\n",
    "    plt.scatter(x[best_idx], y[best_idx], c=\"r\", linewidths=0.1)\n",
    "    plt.title(\"TE-NAS\")\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    for k, v in results.items():\n",
    "        print(k)\n",
    "        best_idx = np.argmax(v)\n",
    "\n",
    "        best_arch, acc = archs[best_idx], api_valid_accs[best_idx]\n",
    "        if api is not None:\n",
    "            print(\"{:}\".format(api.query_by_arch(best_arch, \"200\")))\n",
    "\n",
    "        x = stats.rankdata(v)\n",
    "        y = stats.rankdata(api_valid_accs)\n",
    "        kendalltau = stats.kendalltau(x, y)\n",
    "        spearmanr = stats.spearmanr(x, y)\n",
    "        pearsonr = stats.pearsonr(x, y)\n",
    "        print(\"{}: {}\\t{}\\t{}\\t\".format(k, kendalltau[0], pearsonr[0], spearmanr[0]))\n",
    "        plt.figure(figsize=(4*fig_scale,3*fig_scale))\n",
    "        plt.scatter(x, y, linewidths=0.1)\n",
    "        best_idx = np.argmax(v)\n",
    "        plt.scatter(x[best_idx], y[best_idx], c=\"r\", linewidths=0.1)\n",
    "        plt.title(\"{}\".format(k))\n",
    "        plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "012f3f63",
   "metadata": {},
   "source": [
    "# ### Confusion matrix\n",
    "\n",
    "# import seaborn as sn\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import copy\n",
    "\n",
    "# metrics = copy.deepcopy(results)\n",
    "\n",
    "# metrics['accuracy'] = api_valid_accs\n",
    "# metrics['complexity'] = api_flops\n",
    "\n",
    "# # keys = ['accuracy', 'expressivity', 'progressivity', 'trainability', 'complexity']\n",
    "# # key_names = ['Acc.', r\"$s^{\\mathcal{E}}$\", r\"$s^{\\mathcal{P}}$\", r\"$s^{\\mathcal{T}}$\", r\"$s^{\\mathcal{C}}$\"]\n",
    "# keys = ['expressivity', 'progressivity', 'trainability', 'complexity']\n",
    "# key_names = [r\"$s^{\\mathcal{E}}$\", r\"$s^{\\mathcal{P}}$\", r\"$s^{\\mathcal{T}}$\", r\"$s^{\\mathcal{C}}$\"]\n",
    "# print(keys)\n",
    "\n",
    "# matrix = np.zeros((len(keys),len(keys)))\n",
    "\n",
    "# for i in range(len(keys)):\n",
    "#     for j in range(len(keys)):\n",
    "#         x = stats.rankdata(metrics[keys[i]])\n",
    "#         y = stats.rankdata(metrics[keys[j]])\n",
    "#         kendalltau = stats.kendalltau(x, y)[0]\n",
    "#         matrix[i,j] = kendalltau\n",
    "\n",
    "# print(matrix)\n",
    "        \n",
    "# df_cm = pd.DataFrame(matrix, index = [i for i in key_names], columns = [i for i in key_names])\n",
    "# plt.figure(figsize=(6,3))\n",
    "# sn.set(font_scale=1.9) # for label size\n",
    "# ax = sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 17}, cmap='GnBu', fmt='.2f') # font size\n",
    "# cbar = ax.collections[0].colorbar\n",
    "# cbar.ax.tick_params(labelsize=16)\n",
    "# plt.yticks(rotation=0) \n",
    "# plt.savefig('confusion_matrix.pdf', bbox_inches = 'tight', pad_inches = 0)\n",
    "# plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "43c638e4",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# #### Visualize scatter plots\n",
    "# def visualize_proxy_cmap(x, y, title, save_name, ref_rank=None):\n",
    "#     if ref_rank is None:\n",
    "#         ref_rank = x\n",
    "#     plt.figure(figsize=(4.5*1.5,3*1.5))\n",
    "#     plt.grid(True, alpha=0.3)\n",
    "#     plt.scatter(x,y, linewidths=0.1, c=ref_rank, cmap='viridis_r')\n",
    "#     plt.xlabel(\"Predicted network ranking\", fontsize=16)\n",
    "#     plt.ylabel(\"Ground-truth network ranking\", fontsize=16)\n",
    "#     plt.xticks(fontsize=14)\n",
    "#     plt.yticks(fontsize=14)\n",
    "#     plt.colorbar()\n",
    "#     plt.title(title, fontsize=20)\n",
    "# #     plt.savefig('{}.pdf'.format(save_name), bbox_inches = 'tight', pad_inches = 0)\n",
    "#     plt.show()\n",
    "    \n",
    "# def visualize_proxy(x, y, title, save_name):\n",
    "#     plt.figure(figsize=(3.8*1.5,3*1.5))\n",
    "#     plt.grid(True, alpha=0.3)\n",
    "#     plt.scatter(x,y, linewidths=0.1, c=\"#140c86\")\n",
    "#     plt.xlabel(\"Predicted network ranking\", fontsize=16)\n",
    "#     plt.ylabel(\"Ground-truth network ranking\", fontsize=16)\n",
    "#     plt.xticks(fontsize=14)\n",
    "#     plt.yticks(fontsize=14)\n",
    "#     plt.title(title, fontsize=20)\n",
    "# #     plt.savefig('{}.pdf'.format(save_name), bbox_inches = 'tight', pad_inches = 0)\n",
    "#     plt.show()\n",
    "\n",
    "# if xargs.zero_shot_score.lower() == 'az_nas':\n",
    "#     rank_agg = None\n",
    "#     l = len(api_flops)\n",
    "#     rank_agg = np.log(stats.rankdata(api_flops) / l)\n",
    "#     for k in results.keys():\n",
    "#         print(k)\n",
    "#         if rank_agg is None:\n",
    "#             rank_agg = np.log( stats.rankdata(results[k]) / l)\n",
    "#         else:\n",
    "#             rank_agg = rank_agg + np.log( stats.rankdata(results[k]) / l)\n",
    "\n",
    "\n",
    "#     best_idx = np.argmax(rank_agg)\n",
    "\n",
    "#     best_arch, acc = archs[best_idx], api_valid_accs[best_idx]\n",
    "#     if api is not None:\n",
    "#         print(\"{:}\".format(api.query_by_arch(best_arch, \"200\")))\n",
    "\n",
    "\n",
    "#     x = stats.rankdata(rank_agg)\n",
    "#     x_agg = x\n",
    "#     y = stats.rankdata(api_valid_accs)\n",
    "#     kendalltau = stats.kendalltau(x, y)\n",
    "#     spearmanr = stats.spearmanr(x, y)\n",
    "#     pearsonr = stats.pearsonr(x, y)\n",
    "#     visualize_proxy(x,y,r\"AZ-NAS ($\\tau$={0:.3f}, $\\rho$={1:.3f})\".format(kendalltau[0], spearmanr[0]), 'AZ-NAS_comp')\n",
    "#     visualize_proxy_cmap(x,y,r\"AZ-NAS ($\\tau$={0:.3f}, $\\rho$={1:.3f})\".format(kendalltau[0], spearmanr[0]), 'AZ-NAS')\n",
    "    \n",
    "#     metrics = {'FLOPs':api_flops}\n",
    "#     for k, v in results.items():\n",
    "#         metrics[k] = v\n",
    "\n",
    "#         print(k)\n",
    "#         best_idx = np.argmax(v)\n",
    "\n",
    "#         best_arch, acc = archs[best_idx], api_valid_accs[best_idx]\n",
    "#         if api is not None:\n",
    "#             print(\"{:}\".format(api.query_by_arch(best_arch, \"200\")))\n",
    "\n",
    "#     title_names = {\"expressivity\":\"Expressivity\",\"progressivity\":\"Progressivity\",\"trainability\":\"Trainability\",\"FLOPs\":\"Complexity\"}\n",
    "#     for k in metrics.keys():\n",
    "#         x = stats.rankdata(metrics[k])\n",
    "#         y = stats.rankdata(api_valid_accs)\n",
    "#         kendalltau = stats.kendalltau(x, y)\n",
    "#         spearmanr = stats.spearmanr(x, y)\n",
    "#         pearsonr = stats.pearsonr(x, y)\n",
    "#         visualize_proxy_cmap(x,y,r\"{0} ($\\tau$={1:.3f}, $\\rho$={2:.3f})\".format(title_names[k],kendalltau[0], spearmanr[0]),title_names[k],x_agg)\n",
    "        \n",
    "# elif xargs.zero_shot_score.lower() == 'te_nas':\n",
    "#     title_names = {\"te_nas\":\"TE-NAS\"}\n",
    "#     rank_agg = None\n",
    "#     for k in results.keys():\n",
    "#         print(k)\n",
    "#         if rank_agg is None:\n",
    "#             rank_agg = stats.rankdata(results[k])\n",
    "#         else:\n",
    "#             rank_agg = rank_agg + stats.rankdata(results[k])\n",
    "#     x = stats.rankdata(rank_agg)\n",
    "#     y = stats.rankdata(api_valid_accs)\n",
    "#     kendalltau = stats.kendalltau(x, y)\n",
    "#     spearmanr = stats.spearmanr(x, y)\n",
    "#     pearsonr = stats.pearsonr(x, y)\n",
    "#     visualize_proxy(x,y,r\"{0} ($\\tau$={1:.3f}, $\\rho$={2:.3f})\".format(title_names['te_nas'],kendalltau[0], spearmanr[0]),title_names['te_nas'])\n",
    "        \n",
    "# else:\n",
    "#     title_names = {\"zico\":\"ZiCo\",\"zen\":\"ZenNAS\",'params':\"#Params\",'naswot':\"NASWOT\",\"synflow\":\"Synflow\",\"gradsign\":\"GradSign\"}\n",
    "#     for k in results.keys():\n",
    "#         print(k)\n",
    "#         x = stats.rankdata(results[k])\n",
    "#         y = stats.rankdata(api_valid_accs)\n",
    "#         kendalltau = stats.kendalltau(x, y)\n",
    "#         spearmanr = stats.spearmanr(x, y)\n",
    "#         pearsonr = stats.pearsonr(x, y)\n",
    "#         visualize_proxy(x,y,r\"{0} ($\\tau$={1:.3f}, $\\rho$={2:.3f})\".format(title_names[k],kendalltau[0], spearmanr[0]),title_names[k])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67623abe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
